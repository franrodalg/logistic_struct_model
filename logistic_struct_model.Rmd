---
title: "Analysing Measurements from Intervention-based Evaluation Studies"
output: html_notebook
---

## Setup

```{r}
if (!require(tidyverse)) {
  install.packages('tidyverse')
  library(tidyverse)
}
if (!require(lme4))
  install.packages('lme4')
if (!require(lmerTest))
  install.packages('lmerTest')
if (!require(modelbased))
  install.packages('modelbased')
if (!require(rbenchmark))
  install.packages('rbenchmark')
```

```{r}
set.seed(1986)
```

## Helper Functions

```{r}
get_ability <- function(x, l, z, w, x_num=2, l_num=2) {
  x <- x / (x_num - 1)
  l <- l / (l_num - 1)
  ability <- (0.1 + (x+1)*0.3 + ((l+1)*0.15) + (l*(1-x))*0.1 -
    (z)*0.1 - (z*l)*0.1 - (w*x)*0.3 - (z*w)*0.05)
  min(0.95, max(0.05, ability))
}
```

## Simulation parameters

```{r}
x_num <- 2
l_num <- 2
k_num <- 40
z_num <- 2
w_num <- 2
N <- 100
```

## Computation of Simulated Observations

```{r}
(abilities <- as_tibble(
    expand.grid(
      X=0:(x_num-1),
      L=0:(l_num-1),
      Z=0:(z_num-1),
      W=0:(w_num-1)
    )
  ) %>%
  rowwise() %>%
  mutate(
    feat_ext=paste0('e', X+1),
    learn_alg=paste0('l', L+1),
    conf_1=paste0('z', rep("'", Z)),
    conf_2=paste0('w', rep("'", W)),
    ability=get_ability(X, L, Z, W)) %>%
  ungroup() %>%
  select(-c(1:4)))


```

```{r}
(observations <- abilities %>% merge(
  tibble(samp=1:k_num) %>%
    mutate(difficulty = rnorm(k_num, 0, 0.1))) %>% 
  group_by(samp, feat_ext, learn_alg, conf_1, conf_2) %>%
  mutate(prob=max(0, min(1, ability*(1-difficulty)))) %>%
  merge(tibble(i=1:N))  %>%
  as_tibble() %>%
  rowwise() %>%
  mutate(y=rbinom(1, 1, prob)) %>%
  ungroup() %>%
  select(samp, everything(), -i))
```

```{r}
(measurements <- observations %>%
  group_by(samp, feat_ext, learn_alg, conf_1, conf_2) %>%
  summarise(y=mean(y)))
```

```{r}
measurements %>%
    ggplot() +
    geom_boxplot(aes(x=feat_ext, y=y, color=learn_alg, fill=learn_alg),
               alpha=0.2, size=0.8) +
    facet_grid(rows=vars(conf_1), cols=vars(conf_2)) +
    coord_cartesian(ylim=c(0,1)) +
    labs(x='Feature Extractor', y='Accuracy') +
    scale_color_discrete(name='Learning\nAlgorithm') +
    scale_fill_discrete(name='Learning\nAlgorithm') +
    theme_bw()
```

## Model Definition

```{r}
model <- y ~ learn_alg*feat_ext*conf_1 + learn_alg*feat_ext*conf_2 + 
  conf_1*conf_2 + (1 | samp)
```

## Linear Modelling

```{r}
linear_fit <- lmerTest::lmer(model, data=measurements)
```

```{r}
summary(linear_fit)
```

## Logistic Modelling

```{r}
logistic_fit <- lme4::glmer(
  model,
  family="binomial",
  data=observations,
  control=lme4::glmerControl(optimizer="bobyqa",
                            optCtrl=list(maxfun=1e6)))
```

```{r}
summary(logistic_fit)
```

## Interpretation

### Convert logistic estimates

```{r}
logit_inv <- function(x) { 1 / (1+exp(-x)) }
```

```{r}
data.frame(estimate=lme4::fixef(logistic_fit)) %>%
    rownames_to_column('parameter') %>%
    mutate(converted=logit_inv(estimate))
```

### Estimated Means

```{r}
modelbased::estimate_means(linear_fit)
```

```{r}
modelbased::estimate_means(logistic_fit)
```


## Benchmarking


```{r}
rbenchmark::benchmark(
  "linear" = {
    l <- lmerTest::lmer(model, data=measurements)},
  "logistic" = {
    l <- lme4::glmer(
      model,
      family="binomial",
      data=observations,
      control=lme4::glmerControl(optimizer="bobyqa",
                                 optCtrl=list(maxfun=1e6)))},
  replications = 10,
  columns = c("test", "replications", "elapsed",
              "relative", "user.self", "sys.self"))

```